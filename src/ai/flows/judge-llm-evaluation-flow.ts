
'use server';
/**
 * @fileOverview A Genkit flow that uses an LLM to judge an input against evaluation parameters
 * and generate summaries for summarization definitions.
 * It can use Genkit-configured models or direct Anthropic/OpenAI clients.
 *
 * - judgeLlmEvaluation - A function that takes prompt text, parameter IDs, and connector info.
 * - JudgeLlmEvaluationInput - The input type.
 * - JudgeLlmEvaluationOutput - The return type (structured evaluation and summaries).
 */

import {ai, anthropicClient, openAIClient} from '@/ai/genkit'; // anthropicClient and openAIClient imported
import {z}from 'genkit';
import type {MessageParam} from '@anthropic-ai/sdk/resources/messages';
import type {ChatCompletionMessageParam} from 'openai/resources/chat/completions';

// Zod schema for the input to the flow and prompt
const JudgeLlmEvaluationInputSchema = z.object({
  fullPromptText: z.string().describe(
    "The complete text provided to the LLM, which includes the content to be evaluated, detailed descriptions of the evaluation parameters and their labels, and definitions for any summarization tasks."
  ),
  evaluationParameterIds: z.array(z.string()).optional().describe(
    "An array of the IDs of all evaluation parameters that the LLM should provide judgments for."
  ),
  summarizationParameterIds: z.array(z.string()).optional().describe(
    "An array of the IDs of all summarization parameters for which the LLM should generate summaries."
  ),
  parameterIdsRequiringRationale: z.array(z.string()).optional().describe(
    "An optional array of evaluation parameter IDs for which a 'rationale' field is mandatory in the output object for that parameter."
  ),
  // For Genkit models:
  modelName: z.string().optional().describe(
    "The Genkit model identifier, e.g., 'googleai/gemini-1.5-pro' or 'anthropic/claude-3-opus-20240229'. If not provided, Genkit's default model will be used."
  ),
  // For direct client usage (like Anthropic or OpenAI):
  modelConnectorProvider: z.string().optional().describe("The provider of the model connector, e.g., 'Anthropic', 'OpenAI', 'Vertex AI'."),
  modelConnectorConfigString: z.string().optional().describe("The JSON string configuration for the model connector, potentially containing the model name for direct client usage."),
});
export type JudgeLlmEvaluationInput = z.infer<typeof JudgeLlmEvaluationInputSchema>;

// This is the TypeScript type for the FINAL output of the exported async function.
// The client component expects this structure.
export type JudgeLlmEvaluationOutput = Record<string, { chosenLabel?: string | null; generatedSummary?: string | null; rationale?: string | null; error?: string }>;

// This is the Zod schema for what the LLM is specifically asked to output.
// It's an array of objects, where each object corresponds to either an eval param or a summarization param.
const LlmOutputArrayItemSchema = z.object({
  parameterId: z.string().describe("The ID of an evaluation parameter OR a summarization parameter."),
  chosenLabel: z.string().optional().nullable().describe("The name of the label chosen by the LLM for this evaluation parameter. Only present for evaluation parameters."),
  generatedSummary: z.string().optional().nullable().describe("The summary generated by the LLM for this summarization parameter. Only present for summarization parameters."),
  rationale: z.string().optional().nullable().describe("An optional explanation for the chosen label, if requested for an evaluation parameter.")
});
const LlmOutputArraySchema = z.array(LlmOutputArrayItemSchema)
  .describe("An array of objects, where each object contains a 'parameterId' and either a 'chosenLabel' (for evaluations) or a 'generatedSummary' (for summarizations), and an optional 'rationale' (for evaluations).");


// This is the ASYNC function that client components will import and call.
export async function judgeLlmEvaluation(
  input: JudgeLlmEvaluationInput
): Promise<JudgeLlmEvaluationOutput> {
  let llmOutputArray: z.infer<typeof LlmOutputArraySchema> | null = null;
  try {
    llmOutputArray = await internalJudgeLlmEvaluationFlow(input);
  } catch (flowExecutionError: any) {
    console.error('internalJudgeLlmEvaluationFlow itself threw an error:', flowExecutionError);
    const errorResults: z.infer<typeof LlmOutputArraySchema> = [];
    const errorMessage = `Flow execution error: ${flowExecutionError.message || 'Unknown flow error'}`;
    input.evaluationParameterIds?.forEach(id => {
      errorResults.push({ parameterId: id, chosenLabel: "ERROR_FLOW_FAILURE", rationale: errorMessage, generatedSummary: undefined });
    });
    input.summarizationParameterIds?.forEach(id => {
      errorResults.push({ parameterId: id, generatedSummary: `ERROR: ${errorMessage}`, chosenLabel: undefined, rationale: undefined });
    });
    llmOutputArray = errorResults;
  }

  const finalOutput: JudgeLlmEvaluationOutput = {};
  if (llmOutputArray) {
    for (const item of llmOutputArray) {
      if (item && typeof item.parameterId === 'string') {
        finalOutput[item.parameterId] = {
          chosenLabel: item.chosenLabel,
          generatedSummary: item.generatedSummary,
          rationale: item.rationale,
          error: (item as any).error, 
        };
        
        if (item.chosenLabel === "ERROR_NO_LLM_OUTPUT" || item.chosenLabel === "ERROR_LLM_CALL_FAILED" || item.chosenLabel === "ERROR_FLOW_FAILURE") {
            finalOutput[item.parameterId].error = item.rationale || "LLM did not return parsable output or call failed.";
        }
        if (item.generatedSummary?.startsWith("ERROR:")) {
            finalOutput[item.parameterId].error = item.generatedSummary;
        }

      } else {
        console.warn('judgeLlmEvaluation: Received an invalid item in LlmOutputArray:', item);
      }
    }
  } else {
     console.warn('judgeLlmEvaluation: LlmOutputArray was null or undefined after flow execution attempt.');
  }
  return finalOutput;
}

const handlebarsPrompt = `
You are an expert evaluator. Analyze the following text based on the criteria described within it.
The text to evaluate is:
\`\`\`text
{{{fullPromptText}}}
\`\`\`

After your analysis, provide a JSON array as your response. Each object in the array must have a "parameterId" key.

- If the "parameterId" refers to an **Evaluation Parameter**:
  - The object MUST include a "chosenLabel" key. The value must be the name of the single most appropriate label you have chosen for that parameter.
  {{#if parameterIdsRequiringRationale.length}}
    For the following evaluation parameter IDs, you MUST also include a "rationale" field, explaining your reasoning for the chosen label:
    {{#each parameterIdsRequiringRationale}}
    - {{this}}
    {{/each}}
    For other evaluation parameter IDs, the "rationale" field is optional.
    {{else}}
    The "rationale" field is optional for all evaluation parameters.
    {{/if}}

- If the "parameterId" refers to a **Summarization Definition/Task**:
  - The object MUST include a "generatedSummary" key. The value must be the textual summary you generated based on the definition for that task.
  - Do NOT include "chosenLabel" or "rationale" for summarization tasks.

The Evaluation Parameter IDs you MUST provide judgments for are (if any):
{{#if evaluationParameterIds.length}}
  {{#each evaluationParameterIds}}
  - {{this}}
  {{/each}}
{{else}}
(No evaluation parameters specified for labeling in this run)
{{/if}}

The Summarization Definition IDs you MUST provide summaries for are (if any):
{{#if summarizationParameterIds.length}}
  {{#each summarizationParameterIds}}
  - {{this}}
  {{/each}}
{{else}}
(No summarization tasks specified for this run)
{{/if}}

Your entire response must be ONLY the JSON array, with no other surrounding text or explanations.
Example of the expected JSON array format:
[
  { "parameterId": "eval_param1_id", "chosenLabel": "Correct" },
  { "parameterId": "eval_param2_id_needs_rationale", "chosenLabel": "Partially_Incorrect", "rationale": "The user mentioned X, but missed Y." },
  { "parameterId": "summary_task_abc_id", "generatedSummary": "The user is asking about their recent order's delivery status and seems frustrated with the standard delivery time." },
  { "parameterId": "eval_param3_id", "chosenLabel": "Effective", "rationale": "This part was very clear." }
]
Ensure your response starts with '[' and ends with ']'. Do not include any other text before or after the JSON array.
`;

// This is the Genkit prompt object, used when provider is not Anthropic or OpenAI
const judgePrompt = ai.definePrompt({
  name: 'judgeLlmEvaluationGenkitPrompt', // Renamed for clarity
  input: { schema: JudgeLlmEvaluationInputSchema },
  output: { schema: LlmOutputArraySchema },
  prompt: handlebarsPrompt,
  config: {
    temperature: 0.3, 
  }
});

// This is the Genkit flow definition. It is NOT exported.
const internalJudgeLlmEvaluationFlow = ai.defineFlow(
  {
    name: 'internalJudgeLlmEvaluationFlow',
    inputSchema: JudgeLlmEvaluationInputSchema,
    outputSchema: LlmOutputArraySchema, // The flow itself will return the array
  },
  async (input): Promise<z.infer<typeof LlmOutputArraySchema>> => {
    if ((!input.evaluationParameterIds || input.evaluationParameterIds.length === 0) && 
        (!input.summarizationParameterIds || input.summarizationParameterIds.length === 0)) {
      console.warn('internalJudgeLlmEvaluationFlow: No evaluation or summarization parameters provided. Returning empty array.');
      return [];
    }

    let output: z.infer<typeof LlmOutputArraySchema> | undefined | null = null;
    let usage: any = null; // To store usage from Genkit or Anthropic/OpenAI
    let errorReason = "LLM did not return a parsable output.";

    // Shared prompt construction logic for direct client calls
    const constructDirectClientPrompt = (providerName: 'Anthropic' | 'OpenAI') => {
        const contentToAnalyze = input.fullPromptText;
        let rationaleInstruction = `The "rationale" field is optional for all evaluation parameters.`;
        if (input.parameterIdsRequiringRationale && input.parameterIdsRequiringRationale.length > 0) {
          rationaleInstruction = `For the following evaluation parameter IDs, you MUST also include a "rationale" field, explaining your reasoning for the chosen label: ${input.parameterIdsRequiringRationale.join(', ')}.\nFor other evaluation parameter IDs, the "rationale" field is optional.`;
        }
        const evalParamIdsList = input.evaluationParameterIds && input.evaluationParameterIds.length > 0 
                                  ? input.evaluationParameterIds.map(id => `- ${id}`).join('\n  ') 
                                  : '(No evaluation parameters specified for labeling in this run)';
        const summarizationParamIdsList = input.summarizationParameterIds && input.summarizationParameterIds.length > 0
                                  ? input.summarizationParameterIds.map(id => `- ${id}`).join('\n  ')
                                  : '(No summarization tasks specified for this run)';

        return `You are an expert evaluator. Analyze the following text based on the criteria described within it.
The text to evaluate is:
\`\`\`text
${contentToAnalyze}
\`\`\`

After your analysis, provide a JSON array as your response. Each object in the array must have a "parameterId" key.

- If the "parameterId" refers to an **Evaluation Parameter**:
  - The object MUST include a "chosenLabel" key. The value must be the name of the single most appropriate label you have chosen for that parameter.
  - ${rationaleInstruction}

- If the "parameterId" refers to a **Summarization Definition/Task**:
  - The object MUST include a "generatedSummary" key. The value must be the textual summary you generated based on the definition for that task.
  - Do NOT include "chosenLabel" or "rationale" for summarization tasks.

The Evaluation Parameter IDs you MUST provide judgments for are (if any):
  ${evalParamIdsList}

The Summarization Definition IDs you MUST provide summaries for are (if any):
  ${summarizationParamIdsList}

Your entire response must be ONLY the JSON array, with no other surrounding text or explanations.
Example of the expected JSON array format:
[
  { "parameterId": "eval_param1_id", "chosenLabel": "Correct" },
  { "parameterId": "eval_param2_id_needs_rationale", "chosenLabel": "Partially_Incorrect", "rationale": "The user mentioned X, but missed Y." },
  { "parameterId": "summary_task_abc_id", "generatedSummary": "The user is asking about their recent order's delivery status and seems frustrated with the standard delivery time." },
  { "parameterId": "eval_param3_id", "chosenLabel": "Effective", "rationale": "This part was very clear." }
]
Ensure your response starts with '[' and ends with ']'. Do not include any other text before or after the JSON array.
`;
    };


    try {
      if (input.modelConnectorProvider === 'Anthropic' && input.modelConnectorConfigString) {
        let anthropicModelName: string | undefined;
        try {
          const connectorConfig = JSON.parse(input.modelConnectorConfigString);
          anthropicModelName = connectorConfig.model;
        } catch (e) {
          throw new Error("Failed to parse Anthropic modelConnectorConfigString.");
        }

        if (!anthropicModelName) { throw new Error("Anthropic model name not found in modelConnectorConfigString."); }
        if (!anthropicClient) { throw new Error("Anthropic client is not initialized. Check ANTHROPIC_API_KEY."); }

        console.log(`Using direct Anthropic client with model: ${anthropicModelName}`);
        const anthropicUserPrompt = constructDirectClientPrompt('Anthropic');
        const messages: MessageParam[] = [{ role: 'user', content: anthropicUserPrompt }];
        
        const response = await anthropicClient.messages.create({ model: anthropicModelName, messages: messages, max_tokens: 4096, temperature: 0.3 });
        const responseText = response.content[0].text;
        console.log('Anthropic raw response text:', responseText.substring(0, 500) + (responseText.length > 500 ? '...' : ''));
        try {
          output = LlmOutputArraySchema.parse(JSON.parse(responseText));
        } catch (parseError: any) {
           console.error("Failed to parse Anthropic JSON response:", parseError, "Raw response:", responseText);
           errorReason = `Failed to parse Anthropic JSON response: ${parseError.message}. Raw: ${responseText.substring(0,100)}`;
           throw new Error(errorReason);
        }
      } else if (input.modelConnectorProvider === 'OpenAI' && input.modelConnectorConfigString) {
        let openAIModelName: string | undefined;
        try {
          const connectorConfig = JSON.parse(input.modelConnectorConfigString);
          openAIModelName = connectorConfig.model;
        } catch (e) {
          throw new Error("Failed to parse OpenAI modelConnectorConfigString.");
        }

        if (!openAIModelName) { throw new Error("OpenAI model name not found in modelConnectorConfigString."); }
        if (!openAIClient) { throw new Error("OpenAI client is not initialized. Check OPENAI_API_KEY."); }
        
        console.log(`Using direct OpenAI client with model: ${openAIModelName}`);
        const openAIUserPrompt = constructDirectClientPrompt('OpenAI');
        const messages: ChatCompletionMessageParam[] = [{ role: 'user', content: openAIUserPrompt }];

        const response = await openAIClient.chat.completions.create({ model: openAIModelName, messages: messages, max_tokens: 4096, temperature: 0.3, response_format: { type: "json_object" } });
        const responseText = response.choices[0]?.message?.content;
        if (!responseText) {
            console.error("OpenAI response content is null or undefined. Full response:", response);
            errorReason = "OpenAI response content was empty.";
            throw new Error(errorReason);
        }
        console.log('OpenAI raw response text:', responseText.substring(0, 500) + (responseText.length > 500 ? '...' : ''));
        try {
          const parsedJson = JSON.parse(responseText);
          if (Array.isArray(parsedJson)) { // Case 1: Correct array
            output = LlmOutputArraySchema.parse(parsedJson);
          } else if (typeof parsedJson === 'object' && parsedJson !== null) {
            // Case 2: OpenAI returns the first object of the array directly
            if ('parameterId' in parsedJson && ('chosenLabel' in parsedJson || 'generatedSummary' in parsedJson)) {
              console.warn("OpenAI returned a single JSON object instead of an array. Wrapping it in an array.");
              try {
                output = LlmOutputArraySchema.parse([parsedJson]); // Wrap the single object
              } catch (singleItemParseError) {
                // Fallback: Check if it's an object containing the array (e.g. { "result": [...] })
                if (Object.keys(parsedJson).length === 1 && Array.isArray(Object.values(parsedJson)[0])) {
                  console.warn("OpenAI returned JSON object (single key wrapper), attempting to extract array from its first value.");
                  output = LlmOutputArraySchema.parse(Object.values(parsedJson)[0]);
                } else { // If neither, then it's an unexpected format
                  throw new Error(`OpenAI returned JSON object but not in the expected array format or a known wrapper format. Object keys: ${Object.keys(parsedJson).join(', ')}`);
                }
              }
            } else if (Object.keys(parsedJson).length === 1 && Array.isArray(Object.values(parsedJson)[0])) {
              // Case 3: OpenAI wraps the array in a single-key object, e.g. {"result": [...]}
              console.warn("OpenAI returned JSON object (single key wrapper), attempting to extract array from its first value.");
              output = LlmOutputArraySchema.parse(Object.values(parsedJson)[0]);
            } else { // Case 4: Unexpected object format
               throw new Error(`OpenAI returned an unexpected JSON object format. Object keys: ${Object.keys(parsedJson).join(', ')}`);
            }
          } else { // Case 5: Not an array, not an object
             throw new Error("OpenAI returned JSON but not in the expected array format or object format.");
          }
        } catch (parseError: any) {
           console.error("Failed to parse OpenAI JSON response:", parseError, "Raw response:", responseText);
           errorReason = `Failed to parse OpenAI JSON response: ${parseError.message}. Raw: ${responseText.substring(0,100)}`;
           throw new Error(errorReason);
        }

      } else {
        console.log(`Using Genkit ai.generate with model: ${input.modelName || 'Genkit Default'}`);
        const result = await judgePrompt(input, { model: input.modelName || undefined });
        output = result.output; 
        usage = result.usage;
      }
    } catch (err: any) {
      console.error(`Error during LLM call (Provider: ${input.modelConnectorProvider || 'Genkit Default'}, Model: ${input.modelName || getProviderModelName(input) || 'N/A'}). Error:`, err);
      errorReason = `LLM call failed: ${err.message || 'Unknown error during LLM call.'}`;
      const errorResults: z.infer<typeof LlmOutputArraySchema> = [];
      input.evaluationParameterIds?.forEach(id => {
        errorResults.push({ parameterId: id, chosenLabel: "ERROR_LLM_CALL_FAILED", rationale: errorReason, generatedSummary: undefined });
      });
      input.summarizationParameterIds?.forEach(id => {
        errorResults.push({ parameterId: id, generatedSummary: `ERROR: ${errorReason}`, chosenLabel: undefined, rationale: undefined });
      });
      return errorResults; 
    }

    if (!output) {
      console.error('LLM did not return a parsable output. Provider:', input.modelConnectorProvider || 'Genkit Default', '. Model used:', input.modelName || getProviderModelName(input) || 'N/A', '. Usage/Error Details (if any):', usage);
      const errorResults: z.infer<typeof LlmOutputArraySchema> = [];
      const rationaleForError = `LLM did not return a parsable output. Provider: ${input.modelConnectorProvider || 'Genkit Default'}. Model: ${input.modelName || getProviderModelName(input) || 'N/A'}. Details: ${JSON.stringify(usage || errorReason || 'N/A')}`;
      input.evaluationParameterIds?.forEach(id => {
        errorResults.push({ parameterId: id, chosenLabel: "ERROR_NO_LLM_OUTPUT", rationale: rationaleForError, generatedSummary: undefined });
      });
      input.summarizationParameterIds?.forEach(id => {
        errorResults.push({ parameterId: id, generatedSummary: `ERROR: ${rationaleForError}`, chosenLabel: undefined, rationale: undefined });
      });
      return errorResults;
    }
    
    return output;
  }
);

// Helper to get model name from config string for logging
function getProviderModelName(input: JudgeLlmEvaluationInput): string | undefined {
    if (!input.modelConnectorConfigString) return undefined;
    try {
        const config = JSON.parse(input.modelConnectorConfigString);
        return config.model;
    } catch {
        return undefined;
    }
}
    
