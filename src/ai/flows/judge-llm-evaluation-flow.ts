
'use server';
/**
 * @fileOverview A Genkit flow that uses an LLM to judge an input against evaluation parameters
 * and generate summaries for summarization definitions.
 *
 * - judgeLlmEvaluation - A function that takes a full prompt, evaluation parameter details,
 *   summarization parameter IDs, and a list of parameters requiring rationale, then calls an LLM.
 * - JudgeLlmEvaluationInput - The input type.
 * - JudgeLlmEvaluationOutput - The return type (structured evaluation and summaries).
 */

import {ai} from '@/ai/genkit';
import {z} from 'genkit';

// Zod schema for the input to the flow and prompt
const JudgeLlmEvaluationInputSchema = z.object({
  fullPromptText: z.string().describe(
    "The complete text provided to the LLM, which includes the content to be evaluated, detailed descriptions of the evaluation parameters and their labels, and definitions for any summarization tasks."
  ),
  evaluationParameterIds: z.array(z.string()).optional().describe(
    "An array of the IDs of all evaluation parameters that the LLM should provide judgments for."
  ),
  summarizationParameterIds: z.array(z.string()).optional().describe(
    "An array of the IDs of all summarization parameters for which the LLM should generate summaries."
  ),
  parameterIdsRequiringRationale: z.array(z.string()).optional().describe(
    "An optional array of evaluation parameter IDs for which a 'rationale' field is mandatory in the output object for that parameter."
  ),
});
export type JudgeLlmEvaluationInput = z.infer<typeof JudgeLlmEvaluationInputSchema>;

// This is the TypeScript type for the FINAL output of the exported async function.
// The client component expects this structure.
export type JudgeLlmEvaluationOutput = Record<string, { chosenLabel?: string; generatedSummary?: string; rationale?: string; error?: string }>;

// This is the Zod schema for what the LLM is specifically asked to output.
// It's an array of objects, where each object corresponds to either an eval param or a summarization param.
const LlmOutputArrayItemSchema = z.object({
  parameterId: z.string().describe("The ID of an evaluation parameter OR a summarization parameter."),
  chosenLabel: z.string().optional().describe("The name of the label chosen by the LLM for this evaluation parameter. Only present for evaluation parameters."),
  generatedSummary: z.string().optional().describe("The summary generated by the LLM for this summarization parameter. Only present for summarization parameters."),
  rationale: z.string().optional().describe("An optional explanation for the chosen label, if requested for an evaluation parameter.")
});
const LlmOutputArraySchema = z.array(LlmOutputArrayItemSchema)
  .describe("An array of objects, where each object contains a 'parameterId' and either a 'chosenLabel' (for evaluations) or a 'generatedSummary' (for summarizations), and an optional 'rationale' (for evaluations).");


// This is the ASYNC function that client components will import and call.
export async function judgeLlmEvaluation(
  input: JudgeLlmEvaluationInput
): Promise<JudgeLlmEvaluationOutput> {
  const llmOutputArray = await internalJudgeLlmEvaluationFlow(input);

  const finalOutput: JudgeLlmEvaluationOutput = {};
  if (llmOutputArray) {
    for (const item of llmOutputArray) {
      if (item && typeof item.parameterId === 'string') {
        finalOutput[item.parameterId] = {
          chosenLabel: item.chosenLabel,
          generatedSummary: item.generatedSummary,
          rationale: item.rationale,
        };
      } else {
        console.warn('judgeLlmEvaluation: Received an invalid item in LlmOutputArray:', item);
      }
    }
  } else {
     console.warn('judgeLlmEvaluation: LlmOutputArray was null or undefined.');
  }
  return finalOutput;
}

const handlebarsPrompt = `
You are an expert evaluator. Analyze the following text based on the criteria described within it.
The text to evaluate is:
\`\`\`text
{{{fullPromptText}}}
\`\`\`

After your analysis, provide a JSON array as your response. Each object in the array must have a "parameterId" key.

- If the "parameterId" refers to an **Evaluation Parameter**:
  - The object MUST include a "chosenLabel" key. The value must be the name of the single most appropriate label you have chosen for that parameter.
  - {{#if parameterIdsRequiringRationale.length}}
    For the following evaluation parameter IDs, you MUST also include a "rationale" field, explaining your reasoning for the chosen label:
    {{#each parameterIdsRequiringRationale}}
    - {{this}}
    {{/each}}
    For other evaluation parameter IDs, the "rationale" field is optional.
    {{else}}
    The "rationale" field is optional for all evaluation parameters.
    {{/if}}

- If the "parameterId" refers to a **Summarization Definition/Task**:
  - The object MUST include a "generatedSummary" key. The value must be the textual summary you generated based on the definition for that task.
  - Do NOT include "chosenLabel" or "rationale" for summarization tasks.

The Evaluation Parameter IDs you MUST provide judgments for are (if any):
{{#if evaluationParameterIds.length}}
  {{#each evaluationParameterIds}}
  - {{this}}
  {{/each}}
{{else}}
(No evaluation parameters specified for labeling in this run)
{{/if}}

The Summarization Definition IDs you MUST provide summaries for are (if any):
{{#if summarizationParameterIds.length}}
  {{#each summarizationParameterIds}}
  - {{this}}
  {{/each}}
{{else}}
(No summarization tasks specified for this run)
{{/if}}

Your entire response must be ONLY the JSON array, with no other surrounding text or explanations.
Example of the expected JSON array format:
[
  { "parameterId": "eval_param1_id", "chosenLabel": "Correct" },
  { "parameterId": "eval_param2_id_needs_rationale", "chosenLabel": "Partially_Incorrect", "rationale": "The user mentioned X, but missed Y." },
  { "parameterId": "summary_task_abc_id", "generatedSummary": "The user is asking about their recent order's delivery status and seems frustrated with the standard delivery time." },
  { "parameterId": "eval_param3_id", "chosenLabel": "Effective", "rationale": "This part was very clear." }
]
`;

const judgePrompt = ai.definePrompt({
  name: 'judgeLlmEvaluationPrompt',
  input: { schema: JudgeLlmEvaluationInputSchema },
  output: { schema: LlmOutputArraySchema },
  prompt: handlebarsPrompt,
  config: {
    temperature: 0.3, // Slightly lower for more deterministic evaluation and summarization
  }
});

// This is the Genkit flow definition. It is NOT exported.
const internalJudgeLlmEvaluationFlow = ai.defineFlow(
  {
    name: 'internalJudgeLlmEvaluationFlow',
    inputSchema: JudgeLlmEvaluationInputSchema,
    outputSchema: LlmOutputArraySchema,
  },
  async (input) => {
    console.log('internalJudgeLlmEvaluationFlow received input:', JSON.stringify(input, null, 2));
    
    if ((!input.evaluationParameterIds || input.evaluationParameterIds.length === 0) && 
        (!input.summarizationParameterIds || input.summarizationParameterIds.length === 0)) {
      console.warn('internalJudgeLlmEvaluationFlow: No evaluation or summarization parameters provided. Returning empty array.');
      return [];
    }

    const { output, usage } = await judgePrompt(input);

    if (!output) {
      console.error('LLM did not return a parsable output matching the LlmOutputArraySchema.');
      // Attempt to return a structured error for each requested parameter
      const errorResults: z.infer<typeof LlmOutputArraySchema> = [];
      input.evaluationParameterIds?.forEach(id => {
        errorResults.push({ parameterId: id, chosenLabel: "ERROR_NO_LLM_OUTPUT" });
      });
      input.summarizationParameterIds?.forEach(id => {
        errorResults.push({ parameterId: id, generatedSummary: "ERROR: LLM did not return a parsable output." });
      });
      return errorResults;
    }
    
    console.log('internalJudgeLlmEvaluationFlow LLM usage:', usage);
    console.log('internalJudgeLlmEvaluationFlow LLM output (array):', JSON.stringify(output, null, 2));
    return output;
  }
);
