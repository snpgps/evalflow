
'use server';
/**
 * @fileOverview A Genkit flow that uses an LLM to judge an input against evaluation parameters
 * and generate summaries for summarization definitions.
 * It can use Genkit-configured models or a direct Anthropic client.
 *
 * - judgeLlmEvaluation - A function that takes prompt text, parameter IDs, and connector info.
 * - JudgeLlmEvaluationInput - The input type.
 * - JudgeLlmEvaluationOutput - The return type (structured evaluation and summaries).
 */

import {ai, anthropicClient} from '@/ai/genkit'; // anthropicClient imported
import {z} from 'genkit';
import type {MessageParam} from '@anthropic-ai/sdk/resources/messages';

// Zod schema for the input to the flow and prompt
const JudgeLlmEvaluationInputSchema = z.object({
  fullPromptText: z.string().describe(
    "The complete text provided to the LLM, which includes the content to be evaluated, detailed descriptions of the evaluation parameters and their labels, and definitions for any summarization tasks."
  ),
  evaluationParameterIds: z.array(z.string()).optional().describe(
    "An array of the IDs of all evaluation parameters that the LLM should provide judgments for."
  ),
  summarizationParameterIds: z.array(z.string()).optional().describe(
    "An array of the IDs of all summarization parameters for which the LLM should generate summaries."
  ),
  parameterIdsRequiringRationale: z.array(z.string()).optional().describe(
    "An optional array of evaluation parameter IDs for which a 'rationale' field is mandatory in the output object for that parameter."
  ),
  // For Genkit models:
  modelName: z.string().optional().describe(
    "The Genkit model identifier, e.g., 'googleai/gemini-1.5-pro' or 'anthropic/claude-3-opus-20240229'. If not provided, Genkit's default model will be used."
  ),
  // For direct client usage (like Anthropic):
  modelConnectorProvider: z.string().optional().describe("The provider of the model connector, e.g., 'Anthropic', 'Vertex AI'."),
  modelConnectorConfigString: z.string().optional().describe("The JSON string configuration for the model connector, potentially containing the model name for direct client usage."),
});
export type JudgeLlmEvaluationInput = z.infer<typeof JudgeLlmEvaluationInputSchema>;

// This is the TypeScript type for the FINAL output of the exported async function.
// The client component expects this structure.
export type JudgeLlmEvaluationOutput = Record<string, { chosenLabel?: string; generatedSummary?: string; rationale?: string; error?: string }>;

// This is the Zod schema for what the LLM is specifically asked to output.
// It's an array of objects, where each object corresponds to either an eval param or a summarization param.
const LlmOutputArrayItemSchema = z.object({
  parameterId: z.string().describe("The ID of an evaluation parameter OR a summarization parameter."),
  chosenLabel: z.string().optional().describe("The name of the label chosen by the LLM for this evaluation parameter. Only present for evaluation parameters."),
  generatedSummary: z.string().optional().describe("The summary generated by the LLM for this summarization parameter. Only present for summarization parameters."),
  rationale: z.string().optional().describe("An optional explanation for the chosen label, if requested for an evaluation parameter.")
});
const LlmOutputArraySchema = z.array(LlmOutputArrayItemSchema)
  .describe("An array of objects, where each object contains a 'parameterId' and either a 'chosenLabel' (for evaluations) or a 'generatedSummary' (for summarizations), and an optional 'rationale' (for evaluations).");


// This is the ASYNC function that client components will import and call.
export async function judgeLlmEvaluation(
  input: JudgeLlmEvaluationInput
): Promise<JudgeLlmEvaluationOutput> {
  let llmOutputArray: z.infer<typeof LlmOutputArraySchema> | null = null;
  try {
    llmOutputArray = await internalJudgeLlmEvaluationFlow(input);
  } catch (flowExecutionError: any) {
    console.error('internalJudgeLlmEvaluationFlow itself threw an error:', flowExecutionError);
    const errorResults: z.infer<typeof LlmOutputArraySchema> = [];
    const errorMessage = `Flow execution error: ${flowExecutionError.message || 'Unknown flow error'}`;
    input.evaluationParameterIds?.forEach(id => {
      errorResults.push({ parameterId: id, chosenLabel: "ERROR_FLOW_FAILURE", rationale: errorMessage, generatedSummary: undefined });
    });
    input.summarizationParameterIds?.forEach(id => {
      errorResults.push({ parameterId: id, generatedSummary: `ERROR: ${errorMessage}`, chosenLabel: undefined, rationale: undefined });
    });
    llmOutputArray = errorResults;
  }

  const finalOutput: JudgeLlmEvaluationOutput = {};
  if (llmOutputArray) {
    for (const item of llmOutputArray) {
      if (item && typeof item.parameterId === 'string') {
        finalOutput[item.parameterId] = {
          chosenLabel: item.chosenLabel,
          generatedSummary: item.generatedSummary,
          rationale: item.rationale,
          error: (item as any).error, 
        };
        
        if (item.chosenLabel === "ERROR_NO_LLM_OUTPUT" || item.chosenLabel === "ERROR_LLM_CALL_FAILED" || item.chosenLabel === "ERROR_FLOW_FAILURE") {
            finalOutput[item.parameterId].error = item.rationale || "LLM did not return parsable output or call failed.";
        }
        if (item.generatedSummary?.startsWith("ERROR:")) {
            finalOutput[item.parameterId].error = item.generatedSummary;
        }

      } else {
        console.warn('judgeLlmEvaluation: Received an invalid item in LlmOutputArray:', item);
      }
    }
  } else {
     console.warn('judgeLlmEvaluation: LlmOutputArray was null or undefined after flow execution attempt.');
  }
  return finalOutput;
}

const handlebarsPrompt = `
You are an expert evaluator. Analyze the following text based on the criteria described within it.
The text to evaluate is:
\`\`\`text
{{{fullPromptText}}}
\`\`\`

After your analysis, provide a JSON array as your response. Each object in the array must have a "parameterId" key.

- If the "parameterId" refers to an **Evaluation Parameter**:
  - The object MUST include a "chosenLabel" key. The value must be the name of the single most appropriate label you have chosen for that parameter.
  {{#if parameterIdsRequiringRationale.length}}
    For the following evaluation parameter IDs, you MUST also include a "rationale" field, explaining your reasoning for the chosen label:
    {{#each parameterIdsRequiringRationale}}
    - {{this}}
    {{/each}}
    For other evaluation parameter IDs, the "rationale" field is optional.
    {{else}}
    The "rationale" field is optional for all evaluation parameters.
    {{/if}}

- If the "parameterId" refers to a **Summarization Definition/Task**:
  - The object MUST include a "generatedSummary" key. The value must be the textual summary you generated based on the definition for that task.
  - Do NOT include "chosenLabel" or "rationale" for summarization tasks.

The Evaluation Parameter IDs you MUST provide judgments for are (if any):
{{#if evaluationParameterIds.length}}
  {{#each evaluationParameterIds}}
  - {{this}}
  {{/each}}
{{else}}
(No evaluation parameters specified for labeling in this run)
{{/if}}

The Summarization Definition IDs you MUST provide summaries for are (if any):
{{#if summarizationParameterIds.length}}
  {{#each summarizationParameterIds}}
  - {{this}}
  {{/each}}
{{else}}
(No summarization tasks specified for this run)
{{/if}}

Your entire response must be ONLY the JSON array, with no other surrounding text or explanations.
Example of the expected JSON array format:
[
  { "parameterId": "eval_param1_id", "chosenLabel": "Correct" },
  { "parameterId": "eval_param2_id_needs_rationale", "chosenLabel": "Partially_Incorrect", "rationale": "The user mentioned X, but missed Y." },
  { "parameterId": "summary_task_abc_id", "generatedSummary": "The user is asking about their recent order's delivery status and seems frustrated with the standard delivery time." },
  { "parameterId": "eval_param3_id", "chosenLabel": "Effective", "rationale": "This part was very clear." }
]
`;

// This is the Genkit prompt object, used when provider is not Anthropic
const judgePrompt = ai.definePrompt({
  name: 'judgeLlmEvaluationGenkitPrompt', // Renamed for clarity
  input: { schema: JudgeLlmEvaluationInputSchema },
  output: { schema: LlmOutputArraySchema },
  prompt: handlebarsPrompt,
  config: {
    temperature: 0.3, 
  }
});

// This is the Genkit flow definition. It is NOT exported.
const internalJudgeLlmEvaluationFlow = ai.defineFlow(
  {
    name: 'internalJudgeLlmEvaluationFlow',
    inputSchema: JudgeLlmEvaluationInputSchema,
    outputSchema: LlmOutputArraySchema, // The flow itself will return the array
  },
  async (input): Promise<z.infer<typeof LlmOutputArraySchema>> => {
    if ((!input.evaluationParameterIds || input.evaluationParameterIds.length === 0) && 
        (!input.summarizationParameterIds || input.summarizationParameterIds.length === 0)) {
      console.warn('internalJudgeLlmEvaluationFlow: No evaluation or summarization parameters provided. Returning empty array.');
      return [];
    }

    let output: z.infer<typeof LlmOutputArraySchema> | undefined | null = null;
    let usage: any = null; // To store usage from Genkit or Anthropic
    let errorReason = "LLM did not return a parsable output.";

    try {
      if (input.modelConnectorProvider === 'Anthropic' && input.modelConnectorConfigString) {
        let anthropicModelName: string | undefined;
        try {
          const connectorConfig = JSON.parse(input.modelConnectorConfigString);
          anthropicModelName = connectorConfig.model;
        } catch (e) {
          throw new Error("Failed to parse Anthropic modelConnectorConfigString.");
        }

        if (!anthropicModelName) {
          throw new Error("Anthropic model name not found in modelConnectorConfigString.");
        }
        if (!anthropicClient) {
          throw new Error("Anthropic client is not initialized. Check ANTHROPIC_API_KEY environment variable and server restart.");
        }

        console.log(`Using direct Anthropic client with model: ${anthropicModelName}`);
        
        const contentToAnalyze = input.fullPromptText; // This is the user's prompt with data + criteria text

        let rationaleInstruction = `The "rationale" field is optional for all evaluation parameters.`;
        if (input.parameterIdsRequiringRationale && input.parameterIdsRequiringRationale.length > 0) {
          rationaleInstruction = `For the following evaluation parameter IDs, you MUST also include a "rationale" field, explaining your reasoning for the chosen label: ${input.parameterIdsRequiringRationale.join(', ')}.\nFor other evaluation parameter IDs, the "rationale" field is optional.`;
        }
        
        const evalParamIdsList = input.evaluationParameterIds && input.evaluationParameterIds.length > 0 
                                  ? input.evaluationParameterIds.map(id => `- ${id}`).join('\n  ') 
                                  : '(No evaluation parameters specified for labeling in this run)';
        const summarizationParamIdsList = input.summarizationParameterIds && input.summarizationParameterIds.length > 0
                                  ? input.summarizationParameterIds.map(id => `- ${id}`).join('\n  ')
                                  : '(No summarization tasks specified for this run)';

        const anthropicUserPrompt = `You are an expert evaluator. Analyze the following text based on the criteria described within it.
The text to evaluate is:
\`\`\`text
${contentToAnalyze}
\`\`\`

After your analysis, provide a JSON array as your response. Each object in the array must have a "parameterId" key.

- If the "parameterId" refers to an **Evaluation Parameter**:
  - The object MUST include a "chosenLabel" key. The value must be the name of the single most appropriate label you have chosen for that parameter.
  - ${rationaleInstruction}

- If the "parameterId" refers to a **Summarization Definition/Task**:
  - The object MUST include a "generatedSummary" key. The value must be the textual summary you generated based on the definition for that task.
  - Do NOT include "chosenLabel" or "rationale" for summarization tasks.

The Evaluation Parameter IDs you MUST provide judgments for are (if any):
  ${evalParamIdsList}

The Summarization Definition IDs you MUST provide summaries for are (if any):
  ${summarizationParamIdsList}

Your entire response must be ONLY the JSON array, with no other surrounding text or explanations.
Example of the expected JSON array format:
[
  { "parameterId": "eval_param1_id", "chosenLabel": "Correct" },
  { "parameterId": "eval_param2_id_needs_rationale", "chosenLabel": "Partially_Incorrect", "rationale": "The user mentioned X, but missed Y." },
  { "parameterId": "summary_task_abc_id", "generatedSummary": "The user is asking about their recent order's delivery status and seems frustrated with the standard delivery time." },
  { "parameterId": "eval_param3_id", "chosenLabel": "Effective", "rationale": "This part was very clear." }
]
Ensure your response starts with '[' and ends with ']'. Do not include any other text before or after the JSON array.
`;

        const messages: MessageParam[] = [{ role: 'user', content: anthropicUserPrompt }];
        
        const response = await anthropicClient.messages.create({
          model: anthropicModelName,
          messages: messages,
          max_tokens: 4096,
          temperature: 0.3, 
        });
        
        const responseText = response.content[0].text;
        console.log('Anthropic raw response text:', responseText.substring(0, 500) + (responseText.length > 500 ? '...' : ''));
        try {
          output = LlmOutputArraySchema.parse(JSON.parse(responseText));
        } catch (parseError: any) {
           console.error("Failed to parse Anthropic JSON response:", parseError, "Raw response:", responseText);
           errorReason = `Failed to parse Anthropic JSON response: ${parseError.message}. Raw: ${responseText.substring(0,100)}`;
           throw new Error(errorReason);
        }

      } else {
        console.log(`Using Genkit ai.generate with model: ${input.modelName || 'Genkit Default'}`);
        const result = await judgePrompt(input, { model: input.modelName || undefined });
        output = result.output; 
        usage = result.usage;
      }
    } catch (err: any) {
      console.error(`Error during LLM call (Provider: ${input.modelConnectorProvider || 'Genkit Default'}, Model: ${input.modelName || 'N/A'}). Error:`, err);
      errorReason = `LLM call failed: ${err.message || 'Unknown error during LLM call.'}`;
      const errorResults: z.infer<typeof LlmOutputArraySchema> = [];
      input.evaluationParameterIds?.forEach(id => {
        errorResults.push({ parameterId: id, chosenLabel: "ERROR_LLM_CALL_FAILED", rationale: errorReason, generatedSummary: undefined });
      });
      input.summarizationParameterIds?.forEach(id => {
        errorResults.push({ parameterId: id, generatedSummary: `ERROR: ${errorReason}`, chosenLabel: undefined, rationale: undefined });
      });
      return errorResults; 
    }

    if (!output) {
      console.error('LLM did not return a parsable output. Provider:', input.modelConnectorProvider || 'Genkit Default', '. Model used:', input.modelName || 'N/A', '. Usage/Error Details (if any):', usage);
      const errorResults: z.infer<typeof LlmOutputArraySchema> = [];
      const rationaleForError = `LLM did not return a parsable output. Provider: ${input.modelConnectorProvider || 'Genkit Default'}. Model: ${input.modelName || 'N/A'}. Details: ${JSON.stringify(usage || errorReason || 'N/A')}`;
      input.evaluationParameterIds?.forEach(id => {
        errorResults.push({ parameterId: id, chosenLabel: "ERROR_NO_LLM_OUTPUT", rationale: rationaleForError, generatedSummary: undefined });
      });
      input.summarizationParameterIds?.forEach(id => {
        errorResults.push({ parameterId: id, generatedSummary: `ERROR: ${rationaleForError}`, chosenLabel: undefined, rationale: undefined });
      });
      return errorResults;
    }
    
    return output;
  }
);

    